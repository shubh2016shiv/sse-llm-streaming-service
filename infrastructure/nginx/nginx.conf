# ============================================================================
# NGINX Load Balancer Configuration for SSE Streaming Application
# ============================================================================
#
# WHAT IS NGINX?
# --------------
# NGINX is a high-performance HTTP server and reverse proxy. In this setup,
# it acts as a load balancer, distributing incoming requests across multiple
# application instances.
#
# WHY USE A LOAD BALANCER?
# -------------------------
# 1. HIGH AVAILABILITY: If one app instance fails, traffic routes to healthy instances
# 2. HORIZONTAL SCALING: Add more app instances to handle increased load
# 3. ZERO DOWNTIME DEPLOYMENTS: Update instances one at a time
# 4. SSL/TLS TERMINATION: Handle HTTPS encryption at the edge
# 5. RATE LIMITING: Protect backend from abuse
# 6. CACHING: Reduce backend load for static content
#
# ARCHITECTURE:
# -------------
#   Client → NGINX (Port 80) → App Instance 1 (Port 8000)
#                            → App Instance 2 (Port 8000)
#                            → App Instance 3 (Port 8000)
#
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: WORKER PROCESS CONFIGURATION
# ----------------------------------------------------------------------------
# WHAT ARE WORKER PROCESSES?
# ---------------------------
# NGINX uses a master process that spawns worker processes. Each worker
# handles client connections independently using an event-driven model.
#
# WHY AUTO?
# ---------
# 'auto' tells NGINX to create one worker per CPU core. This maximizes
# performance by utilizing all available CPU resources.
#
# EXAMPLE: On a 4-core machine, NGINX creates 4 worker processes.
# Each worker can handle thousands of concurrent connections.

user nginx;
worker_processes auto;

# ERROR LOG CONFIGURATION
# -----------------------
# Log Level Options (from most to least verbose):
# - debug: Everything (use only for troubleshooting)
# - info: Informational messages
# - notice: Normal but significant events
# - warn: Warning messages (recommended for production)
# - error: Error messages only
# - crit: Critical issues only
# - alert: Immediate action required
# - emerg: System is unusable
#
# WHY 'warn'?
# -----------
# Balances between having enough information for debugging and not
# overwhelming logs with routine operations.

error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

# ----------------------------------------------------------------------------
# STEP 2: EVENT PROCESSING CONFIGURATION
# ----------------------------------------------------------------------------
# WHAT IS THE EVENTS BLOCK?
# --------------------------
# Configures how NGINX handles connections at the OS level. This is critical
# for performance tuning.

events {
    # WORKER CONNECTIONS
    # ------------------
    # Maximum number of simultaneous connections each worker can handle.
    #
    # CALCULATION:
    # Total max connections = worker_processes × worker_connections
    # Example: auto (4 cores) × 1024 = 4096 concurrent connections
    #
    # WHY 1024?
    # ---------
    # Good default for most applications. Can be increased to 2048 or 4096
    # for high-traffic scenarios, but requires OS tuning (ulimit).
    #
    # IMPORTANT: Each connection consumes memory. Monitor system resources
    # when increasing this value.
    
    worker_connections 1024;
    
    # CONNECTION PROCESSING METHOD
    # ----------------------------
    # 'epoll' is the most efficient method on Linux. It allows NGINX to
    # monitor thousands of connections with minimal CPU overhead.
    #
    # ALTERNATIVES:
    # - kqueue: For BSD/macOS
    # - eventport: For Solaris
    # - /dev/poll: For Solaris
    # - select: Fallback (least efficient)
    #
    # NGINX automatically selects the best method, but we explicitly set
    # epoll for clarity and to ensure optimal performance on Linux.
    
    use epoll;
    
    # ACCEPT MULTIPLE CONNECTIONS
    # ----------------------------
    # When enabled, each worker accepts as many connections as possible
    # from the accept queue, rather than one at a time.
    #
    # BENEFIT: Reduces latency during traffic spikes by accepting all
    # pending connections immediately.
    
    multi_accept on;
}

# ----------------------------------------------------------------------------
# STEP 3: HTTP SERVER CONFIGURATION
# ----------------------------------------------------------------------------
# WHAT IS THE HTTP BLOCK?
# ------------------------
# Contains all HTTP-related configuration, including server blocks,
# upstream definitions, and global HTTP settings.

http {
    # ========================================================================
    # MIME TYPES AND DEFAULT CONTENT TYPE
    # ========================================================================
    # WHAT ARE MIME TYPES?
    # --------------------
    # MIME types tell browsers how to handle different file types.
    # Example: text/html for HTML, application/json for JSON.
    #
    # The mime.types file contains mappings like:
    #   .html → text/html
    #   .json → application/json
    #   .css → text/css
    
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # ========================================================================
    # LOGGING CONFIGURATION
    # ========================================================================
    # ACCESS LOG FORMAT
    # -----------------
    # Defines what information to log for each request.
    #
    # VARIABLES EXPLAINED:
    # - $remote_addr: Client IP address
    # - $remote_user: Authenticated username (if any)
    # - $time_local: Request timestamp in local time
    # - $request: Full request line (GET /path HTTP/1.1)
    # - $status: HTTP response status code (200, 404, 500, etc.)
    # - $body_bytes_sent: Response body size in bytes
    # - $http_referer: Referrer URL (where request came from)
    # - $http_user_agent: Client user agent (browser, curl, etc.)
    # - $http_x_forwarded_for: Original client IP (if behind proxy)
    # - $request_time: Total request processing time in seconds
    # - $upstream_addr: Backend server that handled the request
    # - $upstream_response_time: Time backend took to respond
    #
    # WHY LOG THESE?
    # --------------
    # - Debugging: Identify slow requests, errors, traffic patterns
    # - Security: Detect attacks, unusual access patterns
    # - Analytics: Understand user behavior, popular endpoints
    # - Performance: Identify bottlenecks in backend services
    
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct=$upstream_connect_time '
                    'uht=$upstream_header_time urt=$upstream_response_time '
                    'upstream=$upstream_addr';
    
    access_log /var/log/nginx/access.log main;
    
    # ========================================================================
    # PERFORMANCE OPTIMIZATIONS
    # ========================================================================
    
    # SENDFILE OPTIMIZATION
    # ---------------------
    # WHAT IS SENDFILE?
    # -----------------
    # Normally, sending a file requires:
    # 1. Read file from disk to kernel buffer
    # 2. Copy from kernel buffer to application buffer
    # 3. Copy from application buffer back to kernel
    # 4. Send from kernel to network socket
    #
    # With sendfile enabled:
    # 1. Read file from disk to kernel buffer
    # 2. Send directly from kernel to network socket
    #
    # BENEFIT: Eliminates unnecessary copying, reducing CPU usage and
    # improving performance for static file serving.
    
    sendfile on;
    
    # TCP_NOPUSH OPTIMIZATION
    # -----------------------
    # WHAT IS TCP_NOPUSH?
    # -------------------
    # When enabled, NGINX waits to send data until it has a full packet,
    # rather than sending small packets immediately.
    #
    # BENEFIT: Reduces network overhead by sending fewer, larger packets.
    # Works with sendfile to optimize file transmission.
    #
    # IMPORTANT: Only effective when sendfile is enabled.
    
    tcp_nopush on;
    
    # TCP_NODELAY OPTIMIZATION
    # ------------------------
    # WHAT IS TCP_NODELAY?
    # --------------------
    # Disables Nagle's algorithm, which buffers small packets to reduce
    # network overhead.
    #
    # WHY DISABLE NAGLE?
    # ------------------
    # For real-time applications like SSE streaming, we want immediate
    # delivery of data chunks, not buffering. This reduces latency.
    #
    # TRADE-OFF: Slightly more network overhead, but much better latency
    # for streaming applications.
    
    tcp_nodelay on;
    
    # KEEPALIVE TIMEOUT
    # -----------------
    # WHAT IS KEEPALIVE?
    # ------------------
    # HTTP keepalive allows multiple requests to reuse the same TCP connection,
    # avoiding the overhead of establishing new connections.
    #
    # WHY 65 SECONDS?
    # ---------------
    # - Long enough to handle multiple requests from the same client
    # - Short enough to free up connections for new clients
    # - Balances resource usage and performance
    #
    # IMPORTANT: For SSE streaming, connections may stay open much longer.
    # The backend application manages these long-lived connections.
    
    keepalive_timeout 65;
    
    # GZIP COMPRESSION
    # ----------------
    # WHAT IS GZIP?
    # -------------
    # Compresses response bodies before sending to clients, reducing
    # bandwidth usage and improving load times.
    #
    # WHY ENABLE?
    # -----------
    # - Reduces bandwidth costs
    # - Faster page loads (especially on slow connections)
    # - Better user experience
    #
    # TRADE-OFF: Slight CPU overhead for compression, but usually worth it.
    
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;  # Compression level (1-9, 6 is good balance)
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/rss+xml font/truetype font/opentype 
               application/vnd.ms-fontobject image/svg+xml;
    
    # GZIP TYPES EXPLAINED:
    # ---------------------
    # Only compress text-based content. Binary files (images, videos) are
    # already compressed and won't benefit from gzip.
    #
    # NOTE: text/html is always compressed (don't need to specify)
    
    # ========================================================================
    # UPSTREAM BACKEND SERVERS
    # ========================================================================
    # WHAT IS AN UPSTREAM?
    # --------------------
    # An upstream defines a group of backend servers that NGINX can
    # load balance across. This is the heart of load balancing.
    #
    # NAMING: 'sse_backend' is the name we'll reference in server blocks.
    
    upstream sse_backend {
        # LOAD BALANCING ALGORITHM
        # -------------------------
        # NGINX supports several algorithms:
        #
        # 1. ROUND ROBIN (default, no directive needed):
        #    Distributes requests evenly across all servers in order.
        #    Example: Request 1 → Server 1, Request 2 → Server 2, etc.
        #    BEST FOR: Servers with similar capacity
        #
        # 2. LEAST CONNECTIONS (least_conn):
        #    Sends requests to server with fewest active connections.
        #    BEST FOR: Requests with varying processing times
        #
        # 3. IP HASH (ip_hash):
        #    Same client IP always goes to same server (session affinity).
        #    BEST FOR: Stateful applications requiring session persistence
        #
        # 4. LEAST TIME (least_time, NGINX Plus only):
        #    Sends to server with lowest average response time.
        #    BEST FOR: Servers with varying performance
        #
        # FOR SSE STREAMING:
        # ------------------
        # We use LEAST CONNECTIONS because:
        # - SSE connections are long-lived
        # - We want to balance active streams, not just request count
        # - Prevents overloading any single instance
        
        least_conn;
        
        # BACKEND SERVER DEFINITIONS
        # --------------------------
        # Each 'server' directive defines a backend application instance.
        #
        # SYNTAX: server <host>:<port> [parameters];
        #
        # PARAMETERS EXPLAINED:
        # ---------------------
        # - max_fails=3: Mark server as down after 3 consecutive failures
        # - fail_timeout=30s: How long to consider server down before retrying
        # - weight=1: Relative weight for load balancing (higher = more traffic)
        #
        # HEALTH CHECK LOGIC:
        # -------------------
        # 1. NGINX sends request to server
        # 2. If it fails, increment failure counter
        # 3. If failures >= max_fails, mark server as down
        # 4. Wait fail_timeout seconds
        # 5. Try server again (if succeeds, reset failure counter)
        #
        # WHY THESE VALUES?
        # -----------------
        # - max_fails=3: Tolerates transient errors but fails fast
        # - fail_timeout=30s: Gives server time to recover without waiting too long
        #
        # DOCKER NETWORKING:
        # ------------------
        # In Docker Compose, services can reference each other by service name.
        # 'app' is the service name in docker-compose.yml.
        # Docker's internal DNS resolves 'app' to the container IP.
        #
        # SCALING:
        # --------
        # When we scale the app service (docker-compose up --scale app=3),
        # Docker creates multiple containers. We'll configure these as
        # separate upstream servers.
        
        # Instance 1
        server app-1:8000 max_fails=3 fail_timeout=30s;
        
        # Instance 2
        server app-2:8000 max_fails=3 fail_timeout=30s;
        
        # Instance 3
        server app-3:8000 max_fails=3 fail_timeout=30s;
        
        # KEEPALIVE CONNECTIONS TO BACKEND
        # ---------------------------------
        # WHAT IS KEEPALIVE?
        # ------------------
        # Maintains a pool of persistent connections to backend servers,
        # avoiding the overhead of establishing new connections for each request.
        #
        # WHY 32?
        # -------
        # - Enough connections to handle concurrent requests efficiently
        # - Not so many that we waste resources
        # - Can be tuned based on traffic patterns
        #
        # BENEFIT: Reduces latency and backend load by reusing connections.
        
        keepalive 32;
        
        # ADVANCED: ACTIVE HEALTH CHECKS (NGINX Plus only)
        # -------------------------------------------------
        # NGINX Plus supports active health checks that proactively test
        # backend health, rather than waiting for failed requests.
        #
        # Example (not used here, requires NGINX Plus):
        # health_check interval=5s fails=3 passes=2 uri=/health/ready;
        #
        # For NGINX Open Source (what we're using):
        # - Health checks are passive (based on request failures)
        # - We rely on max_fails and fail_timeout
        # - Application provides /health/ready endpoint for external monitoring
    }
    
    # ========================================================================
    # SERVER BLOCK - MAIN LOAD BALANCER
    # ========================================================================
    # WHAT IS A SERVER BLOCK?
    # -----------------------
    # Defines how NGINX handles requests for a specific domain/port.
    # Similar to a virtual host in Apache.
    
    # ============================================================================
    # HTTP TO HTTPS REDIRECT SERVER
    # ============================================================================
    # WHY REDIRECT TO HTTPS?
    # ----------------------
    # - Security best practice: Encrypt all traffic
    # - Modern browsers expect HTTPS
    # - Search engines favor HTTPS sites
    # - Protects against man-in-the-middle attacks

    server {
        listen 80;
        server_name _;

        # REDIRECT ALL HTTP TO HTTPS
        # --------------------------
        # Return 301 (permanent redirect) to HTTPS version
        # $request_uri preserves the original path and query string

        return 301 https://$host$request_uri;
    }

    # ============================================================================
    # HTTPS LOAD BALANCER SERVER
    # ============================================================================

    server {
        # LISTEN ON HTTPS PORT
        # --------------------
        # Port 443 is the standard HTTPS port
        # ssl directive enables SSL/TLS encryption

        listen 443 ssl http2;

        # SSL CERTIFICATES
        # ----------------
        # Point to your SSL certificate and private key
        # In production, use certificates from a trusted CA

        ssl_certificate /etc/nginx/ssl/localhost.crt;
        ssl_certificate_key /etc/nginx/ssl/localhost.key;

        # SSL SECURITY SETTINGS
        # ---------------------
        # Modern SSL/TLS configuration for security
        # These settings provide strong encryption

        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;

        # SERVER NAME
        # -----------
        # WHAT IS THIS?
        # -------------
        # The domain name(s) this server block responds to.
        #
        # '_' is a catch-all that matches any domain.
        #
        # PRODUCTION EXAMPLE:
        # server_name api.example.com streaming.example.com;

        server_name _;
        
        # CLIENT BODY SIZE LIMIT
        # ----------------------
        # WHAT IS THIS?
        # -------------
        # Maximum size of request body (POST/PUT data).
        #
        # WHY 10M?
        # --------
        # - Allows reasonably large requests
        # - Prevents abuse (huge uploads)
        # - Can be adjusted based on application needs
        #
        # FOR SSE APPLICATION:
        # --------------------
        # Requests are typically small (query text), so 10M is generous.
        
        client_max_body_size 10m;
        
        # CLIENT BODY TIMEOUT
        # -------------------
        # WHAT IS THIS?
        # -------------
        # How long to wait for client to send request body.
        #
        # WHY 60s?
        # --------
        # - Long enough for slow connections
        # - Short enough to prevent resource exhaustion
        
        client_body_timeout 60s;
        
        # ====================================================================
        # LOCATION BLOCK - PROXY ALL REQUESTS TO BACKEND
        # ====================================================================
        # WHAT IS A LOCATION BLOCK?
        # --------------------------
        # Defines how to handle requests matching a specific URI pattern.
        #
        # '/' matches all requests (catch-all).
        
        location / {
            # PROXY PASS
            # ----------
            # WHAT DOES THIS DO?
            # ------------------
            # Forwards requests to the upstream backend servers.
            #
            # 'http://sse_backend' references the upstream block we defined.
            #
            # NGINX will:
            # 1. Select a backend server (using least_conn algorithm)
            # 2. Forward the request to that server
            # 3. Return the response to the client
            
            proxy_pass http://sse_backend;
            
            # ================================================================
            # PROXY HEADERS - PRESERVING CLIENT INFORMATION
            # ================================================================
            # WHY SET THESE HEADERS?
            # ----------------------
            # When NGINX proxies requests, the backend sees NGINX's IP, not
            # the original client's IP. These headers preserve client info.
            
            # X-Real-IP
            # ---------
            # The actual client IP address.
            # Backend can use this for logging, rate limiting, geolocation.
            
            proxy_set_header X-Real-IP $remote_addr;
            
            # X-Forwarded-For
            # ---------------
            # WHAT IS THIS?
            # -------------
            # A list of all proxy IPs the request passed through.
            #
            # FORMAT: client_ip, proxy1_ip, proxy2_ip
            #
            # WHY APPEND?
            # -----------
            # If request already has X-Forwarded-For (from CDN, etc.),
            # we append to it rather than replacing.
            
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            
            # X-Forwarded-Proto
            # -----------------
            # WHAT IS THIS?
            # -------------
            # The original protocol (http or https).
            #
            # WHY IMPORTANT?
            # --------------
            # Backend needs to know if original request was HTTPS to:
            # - Generate correct URLs in responses
            # - Enforce HTTPS-only cookies
            # - Redirect HTTP to HTTPS
            
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Host Header
            # -----------
            # WHAT IS THIS?
            # -------------
            # The original Host header from client request.
            #
            # WHY IMPORTANT?
            # --------------
            # Backend may serve different content based on hostname.
            # Preserves virtual host routing.
            
            proxy_set_header Host $host;
            
            # ================================================================
            # HTTP VERSION AND CONNECTION HANDLING
            # ================================================================
            
            # HTTP VERSION
            # ------------
            # WHAT IS THIS?
            # -------------
            # Forces NGINX to use HTTP/1.1 when talking to backend.
            #
            # WHY HTTP/1.1?
            # -------------
            # - Supports keepalive connections (reuse connections)
            # - Required for SSE streaming
            # - More efficient than HTTP/1.0
            
            proxy_http_version 1.1;
            
            # CONNECTION HEADER
            # -----------------
            # WHAT IS THIS?
            # -------------
            # Clears the Connection header to enable keepalive.
            #
            # WHY EMPTY STRING?
            # -----------------
            # HTTP/1.0 clients send "Connection: close", which would
            # prevent keepalive. We clear it to enable connection reuse.
            
            proxy_set_header Connection "";
            
            # ================================================================
            # TIMEOUT CONFIGURATION
            # ================================================================
            # CRITICAL FOR SSE STREAMING!
            #
            # SSE connections are long-lived (minutes to hours). We need
            # generous timeouts to prevent NGINX from closing connections.
            
            # PROXY CONNECT TIMEOUT
            # ---------------------
            # WHAT IS THIS?
            # -------------
            # How long to wait when establishing connection to backend.
            #
            # WHY 60s?
            # --------
            # - Backend should respond quickly (usually < 1s)
            # - 60s allows for backend startup delays
            # - Prevents hanging on dead backends
            
            proxy_connect_timeout 60s;
            
            # PROXY SEND TIMEOUT
            # ------------------
            # WHAT IS THIS?
            # -------------
            # How long to wait when sending request to backend.
            #
            # WHY 60s?
            # --------
            # - Sending request should be fast
            # - Allows for slow network conditions
            
            proxy_send_timeout 60s;
            
            # PROXY READ TIMEOUT
            # ------------------
            # WHAT IS THIS?
            # -------------
            # How long to wait for backend to send data.
            #
            # WHY 300s (5 MINUTES)?
            # ---------------------
            # CRITICAL FOR SSE!
            # - SSE streams can have long pauses between chunks
            # - LLM generation can take time (thinking, processing)
            # - 5 minutes allows for complex queries
            #
            # IMPORTANT: This is reset each time backend sends data.
            # If backend sends a chunk every 30s, connection stays open.
            
            proxy_read_timeout 300s;
            
            # ================================================================
            # BUFFERING CONFIGURATION
            # ================================================================
            # CRITICAL FOR SSE STREAMING!
            
            # PROXY BUFFERING
            # ---------------
            # WHAT IS BUFFERING?
            # ------------------
            # Normally, NGINX buffers the entire backend response before
            # sending to client. This allows NGINX to free backend connection
            # quickly and handle slow clients efficiently.
            #
            # WHY DISABLE FOR SSE?
            # --------------------
            # SSE requires immediate delivery of chunks as they arrive.
            # Buffering would defeat the purpose of streaming!
            #
            # TRADE-OFF:
            # - Disabling buffering ties up backend connection longer
            # - But enables real-time streaming
            # - For SSE, real-time delivery is the priority
            
            proxy_buffering off;
            
            # X-Accel-Buffering Header
            # ------------------------
            # WHAT IS THIS?
            # -------------
            # Tells NGINX not to buffer this specific response.
            #
            # WHY SET THIS?
            # -------------
            # - Reinforces proxy_buffering off
            # - Backend can also set this header to control buffering
            # - Ensures no buffering at any layer
            
            proxy_set_header X-Accel-Buffering no;
            
            # CACHE CONTROL
            # -------------
            # WHAT IS THIS?
            # -------------
            # Tells NGINX and clients not to cache SSE responses.
            #
            # WHY NO CACHE?
            # -------------
            # - SSE responses are real-time and unique
            # - Caching would serve stale data
            # - Each request should get fresh stream
            #
            # NOTE: Application-level caching (Redis) is separate and still works.
            
            proxy_set_header Cache-Control "no-cache, no-store, must-revalidate";
            
            # ================================================================
            # ADDITIONAL OPTIMIZATIONS
            # ================================================================
            
            # PROXY REDIRECT
            # --------------
            # WHAT IS THIS?
            # -------------
            # Rewrites Location and Refresh headers in backend responses.
            #
            # WHY DEFAULT?
            # ------------
            # Ensures redirects use the correct public-facing URL,
            # not internal backend URLs.
            
            proxy_redirect default;
            
            # REQUEST BUFFERING
            # -----------------
            # WHAT IS THIS?
            # -------------
            # Whether to buffer client request body before sending to backend.
            #
            # WHY OFF?
            # --------
            # - Reduces latency (start sending immediately)
            # - Reduces memory usage
            # - Better for streaming uploads
            
            proxy_request_buffering off;
        }
        
        # ====================================================================
        # HEALTH CHECK ENDPOINT FOR LOAD BALANCER ITSELF
        # ====================================================================
        # WHAT IS THIS?
        # -------------
        # A simple endpoint that returns 200 OK to indicate NGINX is healthy.
        #
        # USE CASES:
        # ----------
        # - Kubernetes liveness probe
        # - External monitoring systems
        # - Load balancer health checks (if NGINX is behind another LB)
        
        location /nginx-health {
            access_log off;  # Don't log health checks (reduces noise)
            return 200 "NGINX is healthy\n";
            add_header Content-Type text/plain;
        }
        
        # ====================================================================
        # STUB STATUS MODULE (NGINX METRICS)
        # ====================================================================
        # WHAT IS THIS?
        # -------------
        # Exposes basic NGINX metrics for monitoring.
        #
        # METRICS PROVIDED:
        # -----------------
        # - Active connections
        # - Total accepted connections
        # - Total handled connections
        # - Total requests
        # - Reading/Writing/Waiting connections
        #
        # SECURITY NOTE:
        # --------------
        # In production, restrict access to this endpoint:
        # allow 10.0.0.0/8;  # Internal network only
        # deny all;
        
        location /nginx-status {
            stub_status on;
            access_log off;
        }
    }
}

# ============================================================================
# CONFIGURATION SUMMARY
# ============================================================================
#
# WHAT WE'VE CONFIGURED:
# ----------------------
# 1. Worker processes: Auto-scaled to CPU cores
# 2. Event handling: Optimized with epoll and multi_accept
# 3. HTTP optimizations: sendfile, tcp_nopush, tcp_nodelay, gzip
# 4. Upstream backend: 3 app instances with least_conn load balancing
# 5. Server block: Listens on port 80, proxies to backend
# 6. Proxy settings: Preserves client info, disables buffering for SSE
# 7. Timeouts: Generous for long-lived SSE connections
# 8. Health checks: Passive health checks with max_fails/fail_timeout
# 9. Monitoring: /nginx-health and /nginx-status endpoints
#
# KEY DESIGN DECISIONS:
# ---------------------
# - LEAST_CONN: Best for long-lived SSE connections
# - BUFFERING OFF: Required for real-time streaming
# - LONG TIMEOUTS: Allows for slow LLM generation
# - KEEPALIVE: Reduces connection overhead
# - GZIP: Compresses text responses (not SSE chunks)
#
# PRODUCTION ENHANCEMENTS:
# ------------------------
# - Add SSL/TLS configuration
# - Implement rate limiting (limit_req_zone)
# - Add access control (allow/deny directives)
# - Configure log rotation
# - Set up monitoring and alerting
# - Implement caching for static content
# - Add security headers (X-Frame-Options, CSP, etc.)
#
# ============================================================================